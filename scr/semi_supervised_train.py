import os
from datetime import datetime
import numpy as np
import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, ConcatDataset
from torchvision.datasets import DatasetFolder
import torch.optim as optim
from tools.hw3_model import resnet18_512
from tools.hw3_common_tools import plot_loss_curves, plot_accuracy_curves, plot_training_curves
from PIL import Image
import torchvision.transforms as transforms


# å®šä¹‰å¯åºåˆ—åŒ–çš„å›¾ç‰‡åŠ è½½å‡½æ•°
def pil_loader(path):
    """ä½¿ç”¨PILåŠ è½½å›¾ç‰‡ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGB')  # ç¡®ä¿å›¾ç‰‡æ˜¯RGBæ ¼å¼


# è®¾ç½®åŸºç¡€è·¯å¾„å’Œè®¾å¤‡
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # è‡ªåŠ¨é€‰æ‹©GPUæˆ–CPU
pin_memory = (device.type == 'cuda')  # å¦‚æœä½¿ç”¨GPUï¼Œå¯ç”¨å†…å­˜é”é¡µåŠ é€Ÿæ•°æ®ä¼ è¾“


class SemiSupervisedTrainer:
    """
    åŠç›‘ç£è®­ç»ƒå™¨ç±»
    ç»“åˆæœ‰æ ‡ç­¾æ•°æ®å’Œæ— æ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæå‡æ¨¡å‹æ€§èƒ½
    """

    def __init__(self, model, train_loader, unlabeled_loader, valid_loader,
                 optimizer, criterion, device, pseudo_threshold=0.9, consistency_weight=0.3):
        # åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
        self.model = model
        self.train_loader = train_loader  # æœ‰æ ‡ç­¾è®­ç»ƒæ•°æ®
        self.unlabeled_loader = unlabeled_loader  # æ— æ ‡ç­¾æ•°æ®
        self.valid_loader = valid_loader  # éªŒè¯æ•°æ®
        self.optimizer = optimizer  # ä¼˜åŒ–å™¨
        self.criterion = criterion  # æŸå¤±å‡½æ•°
        self.device = device  # è®­ç»ƒè®¾å¤‡
        self.pseudo_threshold = pseudo_threshold  # ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦é˜ˆå€¼
        self.consistency_weight = consistency_weight  # ä¸€è‡´æ€§æŸå¤±æƒé‡

        # æ•°æ®å¢å¼ºï¼ˆç”¨äºä¸€è‡´æ€§è®­ç»ƒï¼‰
        # å¼±å¢å¼ºï¼šè½»å¾®çš„æ•°æ®å˜æ¢ï¼Œä¿æŒå›¾åƒä¸»è¦å†…å®¹ä¸å˜
        self.weak_augment = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),  # éšæœºæ°´å¹³ç¿»è½¬
            transforms.RandomRotation(degrees=10),  # éšæœºæ—‹è½¬
            transforms.ColorJitter(brightness=0.2, contrast=0.2),  # é¢œè‰²æŠ–åŠ¨
        ])

        # å¼ºå¢å¼ºï¼šæ›´å¼ºçš„æ•°æ®å˜æ¢ï¼Œäº§ç”Ÿæ›´å¤šæ ·åŒ–çš„å›¾åƒ
        self.strong_augment = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),  # æ›´å¼ºçš„é¢œè‰²æŠ–åŠ¨
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # ä»¿å°„å˜æ¢
            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # é«˜æ–¯æ¨¡ç³Š
        ])

    def generate_pseudo_labels(self, confidence_threshold=None):
        """ç”Ÿæˆä¼ªæ ‡ç­¾ï¼šä½¿ç”¨å½“å‰æ¨¡å‹å¯¹æ— æ ‡ç­¾æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œé€‰æ‹©é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹ä½œä¸ºä¼ªæ ‡ç­¾"""
        if confidence_threshold is None:
            confidence_threshold = self.pseudo_threshold

        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        pseudo_data = []  # å­˜å‚¨ä¼ªæ ‡ç­¾æ•°æ®
        pseudo_labels = []  # å­˜å‚¨ä¼ªæ ‡ç­¾

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
            for data, _ in self.unlabeled_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                probabilities = F.softmax(outputs, dim=1)  # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                max_probs, predictions = torch.max(probabilities, 1)  # è·å–æœ€å¤§æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«

                # ç­›é€‰é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼šåªé€‰æ‹©ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„é¢„æµ‹
                mask = max_probs > confidence_threshold
                high_conf_data = data[mask].cpu()  # é«˜ç½®ä¿¡åº¦æ•°æ®
                high_conf_preds = predictions[mask].cpu()  # å¯¹åº”çš„ä¼ªæ ‡ç­¾

                if len(high_conf_data) > 0:
                    for i in range(len(high_conf_data)):
                        pseudo_data.append(high_conf_data[i])
                        pseudo_labels.append(high_conf_preds[i])

        print(f"ç”Ÿæˆäº† {len(pseudo_data)} ä¸ªä¼ªæ ‡ç­¾æ ·æœ¬ (é˜ˆå€¼: {confidence_threshold})")
        return pseudo_data, pseudo_labels

    def consistency_loss(self, unlabeled_batch):
        """è®¡ç®—ä¸€è‡´æ€§æŸå¤±ï¼šå¯¹åŒä¸€æ— æ ‡ç­¾æ•°æ®åº”ç”¨ä¸åŒå¢å¼ºï¼ŒæœŸæœ›æ¨¡å‹è¾“å‡ºä¸€è‡´çš„é¢„æµ‹"""
        batch_size = unlabeled_batch.size(0)

        # å¼±å¢å¼ºï¼šä¿æŒå›¾åƒä¸»è¦å†…å®¹
        weak_aug = self.weak_augment(unlabeled_batch)

        # å¼ºå¢å¼ºï¼šæ›´å¼ºçš„å›¾åƒå˜æ¢
        strong_aug = self.strong_augment(unlabeled_batch)

        # è·å–é¢„æµ‹
        with torch.no_grad():
            weak_output = F.softmax(self.model(weak_aug), dim=1)  # å¼±å¢å¼ºçš„é¢„æµ‹ä½œä¸º"æ•™å¸ˆ"

        strong_output = F.log_softmax(self.model(strong_aug), dim=1)  # å¼ºå¢å¼ºçš„é¢„æµ‹ä½œä¸º"å­¦ç”Ÿ"

        # è®¡ç®—KLæ•£åº¦æŸå¤±ï¼šè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚
        consistency_loss = F.kl_div(strong_output, weak_output, reduction='batchmean')
        return consistency_loss

    def train_epoch(self, epoch, use_consistency=True, use_pseudo_labels=False):
        """è®­ç»ƒä¸€ä¸ªepochï¼šç»“åˆæœ‰ç›‘ç£æŸå¤±å’Œæ— ç›‘ç£æŸå¤±"""
        self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        train_loss = 0.0  # ç´¯è®¡è®­ç»ƒæŸå¤±
        train_correct = 0  # æ­£ç¡®é¢„æµ‹æ•°é‡
        train_total = 0  # æ€»æ ·æœ¬æ•°é‡

        # ä¼ªæ ‡ç­¾ç”Ÿæˆï¼šå®šæœŸä½¿ç”¨å½“å‰æ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾æ¥æ‰©å±•è®­ç»ƒé›†
        if use_pseudo_labels and epoch % 5 == 0 and epoch > 10:
            pseudo_data, pseudo_labels = self.generate_pseudo_labels()
            if pseudo_data:
                # åˆ›å»ºä¼ªæ ‡ç­¾æ•°æ®é›†ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åˆ›å»ºå®Œæ•´çš„Datasetï¼‰
                pseudo_dataset = list(zip(pseudo_data, pseudo_labels))
                print("ä½¿ç”¨ä¼ªæ ‡ç­¾æ•°æ®æ‰©å±•è®­ç»ƒé›†")

        # åˆ›å»ºæ— æ ‡ç­¾æ•°æ®è¿­ä»£å™¨
        unlabeled_iter = iter(self.unlabeled_loader)

        # éå†æœ‰æ ‡ç­¾è®­ç»ƒæ•°æ®
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)

            self.optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦

            # æœ‰ç›‘ç£æŸå¤±ï¼šä½¿ç”¨æœ‰æ ‡ç­¾æ•°æ®è®¡ç®—çš„æ ‡å‡†äº¤å‰ç†µæŸå¤±
            output = self.model(data)
            supervised_loss = self.criterion(output, target)

            total_loss = supervised_loss  # æ€»æŸå¤±åˆå§‹åŒ–ä¸ºæœ‰ç›‘ç£æŸå¤±

            # æ— ç›‘ç£æŸå¤±ï¼ˆä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼‰ï¼šä½¿ç”¨æ— æ ‡ç­¾æ•°æ®è®¡ç®—çš„ä¸€è‡´æ€§æŸå¤±
            if use_consistency:
                try:
                    unlabeled_data, _ = next(unlabeled_iter)
                    unlabeled_data = unlabeled_data.to(self.device)

                    consistency_loss = self.consistency_loss(unlabeled_data)
                    # ç»„åˆæŸå¤±ï¼šæœ‰ç›‘ç£æŸå¤± + æƒé‡ * æ— ç›‘ç£æŸå¤±
                    total_loss = supervised_loss + self.consistency_weight * consistency_loss

                except StopIteration:
                    # é‡ç½®è¿­ä»£å™¨ï¼šå½“æ— æ ‡ç­¾æ•°æ®éå†å®Œæ—¶é‡æ–°å¼€å§‹
                    unlabeled_iter = iter(self.unlabeled_loader)
                    consistency_loss = torch.tensor(0.0)

            total_loss.backward()  # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            self.optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°

            train_loss += total_loss.item()  # ç´¯è®¡æŸå¤±

            # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
            _, predicted = torch.max(output.data, 1)
            train_total += target.size(0)
            train_correct += (predicted == target).sum().item()

            # å®šæœŸæ‰“å°è®­ç»ƒä¿¡æ¯
            if batch_idx % 50 == 0:
                cons_loss_val = consistency_loss.item() if use_consistency else 0.0
                print(f'Epoch: {epoch} | Batch: {batch_idx}/{len(self.train_loader)} | '
                      f'Total Loss: {total_loss.item():.4f} | Supervised: {supervised_loss.item():.4f} | '
                      f'Consistency: {cons_loss_val:.4f}')

        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡
        avg_train_loss = train_loss / len(self.train_loader)
        train_accuracy = 100.0 * train_correct / train_total

        return avg_train_loss, train_accuracy

    def validate(self):
        """éªŒè¯æ¨¡å‹æ€§èƒ½ï¼šåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            for data, target in self.valid_loader:
                data, target = data.to(self.device), target.to(self.device)

                output = self.model(data)
                loss = self.criterion(output, target)

                val_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()

        avg_val_loss = val_loss / len(self.valid_loader)
        val_accuracy = 100.0 * val_correct / val_total

        return avg_val_loss, val_accuracy


def main():
    """ä¸»å‡½æ•°ï¼šé…ç½®å‚æ•°ã€åŠ è½½æ•°æ®ã€è®­ç»ƒæ¨¡å‹"""
    # ============================ é…ç½®å‚æ•° ============================
    parent_dir = os.path.dirname(BASE_DIR)  # è·å–ä¸Šçº§ç›®å½•

    # åˆ›å»ºæ—¥å¿—ç›®å½•ï¼šä»¥å½“å‰æ—¶é—´å‘½åï¼Œé¿å…é‡å¤
    now_time = datetime.now()
    time_str = datetime.strftime(now_time, '%m-%d_%H-%M')
    log_dir = os.path.join(BASE_DIR, "..", "results", "semi_supervised_" + time_str)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # è®­ç»ƒè¶…å‚æ•°
    MAX_EPOCH = 182  # æœ€å¤§è®­ç»ƒè½®æ•°
    BATCH_SIZE = 32  # æ‰¹å¤§å°
    LR = 0.001  # å­¦ä¹ ç‡
    PATIENCE = 20  # æ—©åœè€å¿ƒå€¼
    milestones = [92, 136]  # å­¦ä¹ ç‡è°ƒæ•´çš„é‡Œç¨‹ç¢‘

    # åŠç›‘ç£å‚æ•°
    PSEUDO_THRESHOLD = 0.9  # ä¼ªæ ‡ç­¾ç½®ä¿¡åº¦é˜ˆå€¼
    CONSISTENCY_WEIGHT = 0.3  # ä¸€è‡´æ€§æŸå¤±æƒé‡

    # ============================ æ•°æ®åŠ è½½ ============================
    # è®­ç»ƒæ•°æ®å¢å¼ºï¼šä½¿ç”¨å¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›
    train_tfm = transforms.Compose([
        transforms.Resize((512, 512)),  # è°ƒæ•´å›¾åƒå¤§å°
        transforms.RandomHorizontalFlip(p=0.5),  # éšæœºæ°´å¹³ç¿»è½¬
        transforms.RandomRotation(degrees=15),  # éšæœºæ—‹è½¬
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),  # é¢œè‰²æŠ–åŠ¨
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # ä»¿å°„å˜æ¢
        transforms.RandomCrop(512, padding=16),  # éšæœºè£å‰ª
        transforms.ToTensor(),  # è½¬æ¢ä¸ºTensor
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # æ ‡å‡†åŒ–
    ])

    # æµ‹è¯•/éªŒè¯æ•°æ®å¢å¼ºï¼šåªè¿›è¡Œå¿…è¦çš„é¢„å¤„ç†
    test_tfm = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # åŠ è½½æ•°æ®é›†
    # æœ‰æ ‡ç­¾è®­ç»ƒé›†
    train_set = DatasetFolder(
        os.path.join(parent_dir, "food-11", "training", "labeled"),
        loader=pil_loader,
        extensions="jpg",
        transform=train_tfm
    )

    # éªŒè¯é›†
    valid_set = DatasetFolder(
        os.path.join(parent_dir, "food-11", "validation"),
        loader=pil_loader,
        extensions="jpg",
        transform=test_tfm
    )

    # æ— æ ‡ç­¾æ•°æ®é›†
    unlabeled_set = DatasetFolder(
        os.path.join(parent_dir, "food-11", "training", "unlabeled"),
        loader=pil_loader,
        extensions="jpg",
        transform=train_tfm  # è®­ç»ƒæ—¶å¢å¼º
    )

    # æµ‹è¯•é›†
    test_set = DatasetFolder(
        os.path.join(parent_dir, "food-11", "testing"),
        loader=pil_loader,
        extensions="jpg",
        transform=test_tfm
    )

    # æ•°æ®åŠ è½½å™¨
    num_workers = 2 if os.name == 'nt' else 4  # Windowsç³»ç»Ÿä½¿ç”¨è¾ƒå°‘è¿›ç¨‹

    train_loader = DataLoader(
        train_set,
        batch_size=BATCH_SIZE,
        shuffle=True,  # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®
        num_workers=num_workers,
        pin_memory=pin_memory
    )

    unlabeled_loader = DataLoader(
        unlabeled_set,
        batch_size=BATCH_SIZE,
        shuffle=True,  # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®
        num_workers=num_workers,
        pin_memory=pin_memory
    )

    valid_loader = DataLoader(
        valid_set,
        batch_size=BATCH_SIZE,
        shuffle=False,  # éªŒè¯æ—¶ä¸éœ€è¦æ‰“ä¹±
        num_workers=num_workers,
        pin_memory=pin_memory
    )

    print(f"è®­ç»ƒé›†: {len(train_set)}")
    print(f"æ— æ ‡ç­¾é›†: {len(unlabeled_set)}")
    print(f"éªŒè¯é›†: {len(valid_set)}")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # ============================ æ¨¡å‹å®šä¹‰ ============================
    model = resnet18_512(num_classes=11)  # ä½¿ç”¨ResNet-18æ¨¡å‹ï¼Œé€‚é…512x512è¾“å…¥
    # æˆ–è€…ä½¿ç”¨: model = FoodCNN_2(num_classes=11)  # è½»é‡çº§è‡ªå®šä¹‰CNN
    model.to(device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡

    print(f"æ¨¡å‹å·²åˆ›å»ºï¼Œç§»åŠ¨åˆ°è®¾å¤‡: {device}")
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # ============================ æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ============================
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)  # AdamWä¼˜åŒ–å™¨
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, gamma=0.1, milestones=milestones)  # å¤šæ­¥å­¦ä¹ ç‡è°ƒåº¦

    # ============================ åˆ›å»ºè®­ç»ƒå™¨ ============================
    trainer = SemiSupervisedTrainer(
        model=model,
        train_loader=train_loader,
        unlabeled_loader=unlabeled_loader,
        valid_loader=valid_loader,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        pseudo_threshold=PSEUDO_THRESHOLD,
        consistency_weight=CONSISTENCY_WEIGHT
    )

    # ============================ è®­ç»ƒå¾ªç¯ ============================
    # åˆå§‹åŒ–è®°å½•å˜é‡
    train_losses = []  # è®­ç»ƒæŸå¤±è®°å½•
    val_losses = []  # éªŒè¯æŸå¤±è®°å½•
    train_accuracies = []  # è®­ç»ƒå‡†ç¡®ç‡è®°å½•
    val_accuracies = []  # éªŒè¯å‡†ç¡®ç‡è®°å½•
    learning_rates = []  # å­¦ä¹ ç‡è®°å½•
    best_val_accuracy = 0.0  # æœ€ä½³éªŒè¯å‡†ç¡®ç‡
    early_stop_counter = 0  # æ—©åœè®¡æ•°å™¨
    best_epoch = 0  # æœ€ä½³æ¨¡å‹æ‰€åœ¨è½®æ•°

    print("å¼€å§‹åŠç›‘ç£è®­ç»ƒ...")

    for epoch in range(MAX_EPOCH):
        # è®­ç»ƒé˜¶æ®µ
        train_loss, train_accuracy = trainer.train_epoch(
            epoch,
            use_consistency=True,  # ä½¿ç”¨ä¸€è‡´æ€§è®­ç»ƒ
            use_pseudo_labels=True  # ä½¿ç”¨ä¼ªæ ‡ç­¾
        )

        train_losses.append(train_loss)
        train_accuracies.append(train_accuracy)

        # æ›´æ–°å­¦ä¹ ç‡å¹¶è®°å½•
        current_lr = scheduler.get_last_lr()[0]
        learning_rates.append(current_lr)
        scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

        # éªŒè¯é˜¶æ®µ
        val_loss, val_accuracy = trainer.validate()
        val_losses.append(val_loss)
        val_accuracies.append(val_accuracy)

        # æ—©åœåˆ¤æ–­å’Œæ¨¡å‹ä¿å­˜
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_epoch = epoch
            early_stop_counter = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            checkpoint = {
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "scheduler_state_dict": scheduler.state_dict(),
                "epoch": epoch,
                "best_val_accuracy": best_val_accuracy,
                "val_loss": val_loss,
                "train_accuracy": train_accuracy,
                "train_loss": train_loss
            }
            path_checkpoint = os.path.join(log_dir, "checkpoint_best.pkl")
            torch.save(checkpoint, path_checkpoint)
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")
        else:
            early_stop_counter += 1  # å¢åŠ æ—©åœè®¡æ•°å™¨

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f'Epoch: {epoch:03d}/{MAX_EPOCH}, '
              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '
              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, '
              f'LR: {current_lr:.6f}, '
              f'EarlyStop: {early_stop_counter}/{PATIENCE}')

        # æ—©åœæ£€æŸ¥ï¼šå¦‚æœè¿ç»­PATIENCEä¸ªepochéªŒè¯å‡†ç¡®ç‡æ²¡æœ‰æå‡ï¼Œåœæ­¢è®­ç»ƒ
        if early_stop_counter >= PATIENCE:
            print(f"ğŸš¨ æ—©åœè§¦å‘ï¼åœ¨ epoch {epoch} åœæ­¢è®­ç»ƒ")
            print(f"ğŸ† æœ€ä½³æ¨¡å‹åœ¨ epoch {best_epoch}, éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")
            break

    # ============================ è®­ç»ƒç»“æŸ ============================
    print(f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%")

    # ä¿å­˜è®­ç»ƒè®°å½•
    training_history = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accuracies': train_accuracies,
        'val_accuracies': val_accuracies,
        'learning_rates': learning_rates,
        'best_val_accuracy': best_val_accuracy,
        'best_epoch': best_epoch
    }
    torch.save(training_history, os.path.join(log_dir, 'training_history.pth'))

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    picture_path_loss = os.path.join(log_dir, 'loss_curves.png')
    picture_path_acc = os.path.join(log_dir, 'accuracy_curves.png')
    picture_path_combined = os.path.join(log_dir, 'training_curves.png')

    plot_loss_curves(train_losses, val_losses, picture_path_loss)
    plot_accuracy_curves(train_accuracies, val_accuracies, picture_path_acc)
    plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, picture_path_combined)

    print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {log_dir}")


if __name__ == "__main__":
    main()  # ç¨‹åºå…¥å£ç‚¹