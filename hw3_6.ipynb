{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  ä¸‹è½½æ•°æ® ",
   "id": "dc76f3f3c2e938b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip\n",
    "\n",
    "# Unzip the dataset.\n",
    "!unzip -q food-11.zip\n"
   ],
   "id": "bd4f38da3c728136"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# å¯¼å…¥åŒ…",
   "id": "c4e1c32f586d904b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import DatasetFolder\n"
   ],
   "id": "9b6fca9aea9a65f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# æ„å»ºResNet-18-512æ¨¡å‹",
   "id": "fb14bf750a975dd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## åŸºç¡€æ®‹å·®å—",
   "id": "31328017e34cf704"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ],
   "id": "ebb8c8db3e4dad6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## é€‚é…512Ã—512çš„ResNet-18æ¨¡å‹",
   "id": "70a7787d719efd34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ResNet18_512(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=11, num_channels=3, dropout_rate=0.5):\n",
    "        super(ResNet18_512, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # åˆå§‹å·ç§¯å±‚ - é’ˆå¯¹512Ã—512è°ƒæ•´\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # æ®‹å·®å±‚\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # åˆ†ç±»å™¨\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # æƒé‡åˆå§‹åŒ–\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bin_channels, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # è¾“å…¥: 512Ã—512\n",
    "        x = self.conv1(x)    # 512â†’256\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)  # 256â†’128\n",
    "\n",
    "        x = self.layer1(x)   # 128â†’128\n",
    "        x = self.layer2(x)   # 128â†’64\n",
    "        x = self.layer3(x)   # 64â†’32\n",
    "        x = self.layer4(x)   # 32â†’16\n",
    "\n",
    "        x = self.avgpool(x)  # 16â†’1\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "id": "fc3c31e07085977e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## åˆ›å»ºæ¨¡å‹çš„å‡½æ•°",
   "id": "374ddeaa5ecf436a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def resnet18_512(num_classes=11):\n",
    "    \"\"\"åˆ›å»ºé€‚é…512Ã—512çš„ResNet-18æ¨¡å‹\"\"\"\n",
    "    return ResNet18_512(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)"
   ],
   "id": "a2ea50c6053e4ba8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# æ·»åŠ å·¥å…·å‡½æ•°",
   "id": "b33472660dc0905b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ç»˜åˆ¶è®­ç»ƒå’ŒæŸå¤±æ›²çº¿",
   "id": "f32d1df708bd61c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_loss_curves(train_losses, val_losses, save_path=None):\n",
    "    \"\"\"ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', linewidth=2)\n",
    "    plt.plot(val_losses, label='éªŒè¯æŸå¤±', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_path}\")"
   ],
   "id": "44ebd96cbe790760"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ç»˜åˆ¶è®­ç»ƒå‡†ç¡®ç‡æ›²çº¿",
   "id": "4415aab72b3fbd42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_accuracy_curves(train_accuracies, val_accuracies, save_path=None):\n",
    "    \"\"\"ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(train_accuracies) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)\n",
    "\n",
    "    plt.title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "    # æ·»åŠ æœ€ä½³å‡†ç¡®ç‡æ ‡æ³¨\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_epoch = val_accuracies.index(best_val_acc) + 1\n",
    "    plt.axvline(x=best_epoch, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.text(best_epoch, best_val_acc / 2, f'æœ€ä½³: {best_val_acc:.2f}%\\nEpoch: {best_epoch}',\n",
    "             ha='center', va='center', fontsize=10, \n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ],
   "id": "c3372b0c90b696a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ç»˜åˆ¶ç»¼åˆæ›²çº¿",
   "id": "71e3c0c5472ce2d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, save_path=None):\n",
    "    \"\"\"ç»˜åˆ¶ç»¼åˆè®­ç»ƒæ›²çº¿\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "    ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)\n",
    "    ax1.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)\n",
    "    ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿\n",
    "    ax2.plot(epochs, train_accuracies, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)\n",
    "    ax2.plot(epochs, val_accuracies, 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)\n",
    "    ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epochs', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.legend(fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(bottom=0)\n",
    "\n",
    "    # æ·»åŠ æœ€ä½³å‡†ç¡®ç‡æ ‡æ³¨\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_epoch = val_accuracies.index(best_val_acc) + 1\n",
    "    ax2.axvline(x=best_epoch, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax2.text(best_epoch, best_val_acc / 2, f'æœ€ä½³: {best_val_acc:.2f}%',\n",
    "             ha='center', va='center', fontsize=10,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ],
   "id": "59d4e469b937e880"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# åŠç›‘ç£å­¦ä¹ è®­ç»ƒå™¨",
   "id": "386a41089f142d88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## å›¾ç‰‡åŠ è½½å™¨",
   "id": "3ee04f54fbb7d397"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# å®šä¹‰å¯åºåˆ—åŒ–çš„å›¾ç‰‡åŠ è½½å‡½æ•°\n",
    "def pil_loader(path):\n",
    "    \"\"\"ä½¿ç”¨PILåŠ è½½å›¾ç‰‡ï¼Œæ”¯æŒå¤šç§æ ¼å¼\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')"
   ],
   "id": "55a988b8cd67ac16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## è®­ç»ƒå™¨ä¸»ä½“ç±»",
   "id": "1f6d70487303289f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SemiSupervisedTrainer:\n",
    "    \"\"\"ä¼˜åŒ–ç‰ˆåŠç›‘ç£è®­ç»ƒå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, unlabeled_loader, valid_loader,\n",
    "                 optimizer, criterion, device, pseudo_threshold=0.9, consistency_weight=0.3):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.unlabeled_loader = unlabeled_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.pseudo_threshold = pseudo_threshold\n",
    "        self.consistency_weight = consistency_weight\n",
    "\n",
    "        # æ•°æ®å¢å¼º\n",
    "        self.weak_augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        ])\n",
    "\n",
    "        self.strong_augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "        ])\n",
    "        \n",
    "        # è®­ç»ƒå†å²è®°å½•\n",
    "        self.pseudo_label_stats = []\n",
    "\n",
    "    def generate_pseudo_labels(self, epoch=None):\n",
    "        \"\"\"ä¼˜åŒ–ç‰ˆä¼ªæ ‡ç­¾ç”Ÿæˆï¼šåŠ¨æ€é˜ˆå€¼è°ƒæ•´\"\"\"\n",
    "        # åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼šå‰æœŸä¸¥æ ¼ï¼ŒåæœŸæ”¾å®½\n",
    "        if epoch < 20:\n",
    "            confidence_threshold = self.pseudo_threshold + 0.03  # æ›´ä¸¥æ ¼\n",
    "        elif epoch < 50:\n",
    "            confidence_threshold = self.pseudo_threshold + 0.01\n",
    "        else:\n",
    "            confidence_threshold = self.pseudo_threshold - 0.02  # ç¨å®½æ¾\n",
    "\n",
    "        self.model.eval()\n",
    "        pseudo_data = []\n",
    "        pseudo_labels = []\n",
    "        pseudo_confidences = []\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, _ in self.unlabeled_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                max_probs, predictions = torch.max(probabilities, 1)\n",
    "                \n",
    "                total_samples += data.size(0)\n",
    "                \n",
    "                # ç­›é€‰é«˜ç½®ä¿¡åº¦æ ·æœ¬\n",
    "                mask = max_probs > confidence_threshold\n",
    "                high_conf_data = data[mask].cpu()\n",
    "                high_conf_preds = predictions[mask].cpu()\n",
    "                high_conf_probs = max_probs[mask].cpu()\n",
    "                \n",
    "                if len(high_conf_data) > 0:\n",
    "                    for i in range(len(high_conf_data)):\n",
    "                        pseudo_data.append(high_conf_data[i])\n",
    "                        pseudo_labels.append(high_conf_preds[i])\n",
    "                        pseudo_confidences.append(high_conf_probs[i].item())\n",
    "\n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        selection_rate = len(pseudo_data) / total_samples if total_samples > 0 else 0\n",
    "        avg_confidence = np.mean(pseudo_confidences) if pseudo_confidences else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch}: ç”Ÿæˆäº† {len(pseudo_data)}/{total_samples} ä¸ªä¼ªæ ‡ç­¾ \"\n",
    "              f\"(é€‰æ‹©ç‡: {selection_rate:.3f}, é˜ˆå€¼: {confidence_threshold:.3f}, \"\n",
    "              f\"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.4f})\")\n",
    "        \n",
    "        # è®°å½•ç»Ÿè®¡ä¿¡æ¯\n",
    "        self.pseudo_label_stats.append({\n",
    "            'epoch': epoch,\n",
    "            'count': len(pseudo_data),\n",
    "            'selection_rate': selection_rate,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'threshold': confidence_threshold\n",
    "        })\n",
    "        \n",
    "        return pseudo_data, pseudo_labels\n",
    "\n",
    "    def get_dynamic_consistency_weight(self, epoch):\n",
    "        \"\"\"åŠ¨æ€è°ƒæ•´ä¸€è‡´æ€§æƒé‡\"\"\"\n",
    "        if epoch < 10:\n",
    "            return self.consistency_weight * 0.3  # å‰æœŸæ³¨é‡æœ‰ç›‘ç£å­¦ä¹ \n",
    "        elif epoch < 30:\n",
    "            return self.consistency_weight * 0.7\n",
    "        elif epoch < 60:\n",
    "            return self.consistency_weight\n",
    "        else:\n",
    "            return self.consistency_weight * 1.2  # åæœŸå¢åŠ æ— ç›‘ç£å­¦ä¹ \n",
    "\n",
    "    def consistency_loss(self, unlabeled_batch):\n",
    "        \"\"\"è®¡ç®—ä¸€è‡´æ€§æŸå¤±\"\"\"\n",
    "        batch_size = unlabeled_batch.size(0)\n",
    "\n",
    "        # å¼±å¢å¼º\n",
    "        weak_aug = self.weak_augment(unlabeled_batch)\n",
    "        # å¼ºå¢å¼º\n",
    "        strong_aug = self.strong_augment(unlabeled_batch)\n",
    "\n",
    "        # è·å–é¢„æµ‹\n",
    "        with torch.no_grad():\n",
    "            weak_output = F.softmax(self.model(weak_aug), dim=1)\n",
    "\n",
    "        strong_output = F.log_softmax(self.model(strong_aug), dim=1)\n",
    "\n",
    "        # è®¡ç®—KLæ•£åº¦æŸå¤±\n",
    "        consistency_loss = F.kl_div(strong_output, weak_output, reduction='batchmean')\n",
    "        return consistency_loss\n",
    "\n",
    "    def calculate_topk_accuracy(self, outputs, targets, k=5):\n",
    "        \"\"\"è®¡ç®—top-kå‡†ç¡®ç‡\"\"\"\n",
    "        _, topk_pred = outputs.topk(k, 1, True, True)\n",
    "        topk_correct = topk_pred.eq(targets.view(-1, 1).expand_as(topk_pred))\n",
    "        return topk_correct.any(1).sum().item()\n",
    "\n",
    "    def train_epoch(self, epoch, use_consistency=True, use_pseudo_labels=False):\n",
    "        \"\"\"ä¼˜åŒ–ç‰ˆè®­ç»ƒepoch\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_top5_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # åŠ¨æ€æƒé‡\n",
    "        current_consistency_weight = self.get_dynamic_consistency_weight(epoch) if use_consistency else 0.0\n",
    "        \n",
    "        # æ”¹è¿›çš„ä¼ªæ ‡ç­¾ç­–ç•¥\n",
    "        pseudo_dataset = None\n",
    "        if use_pseudo_labels and epoch % 5 == 0 and epoch >= 10:\n",
    "            pseudo_data, pseudo_labels = self.generate_pseudo_labels(epoch=epoch)\n",
    "            if pseudo_data and len(pseudo_data) > 50:  # åªæœ‰è¶³å¤Ÿå¤šçš„ä¼ªæ ‡ç­¾æ—¶æ‰ä½¿ç”¨\n",
    "                pseudo_dataset = list(zip(pseudo_data, pseudo_labels))\n",
    "                print(f\"ä½¿ç”¨ {len(pseudo_data)} ä¸ªä¼ªæ ‡ç­¾æ ·æœ¬æ‰©å±•è®­ç»ƒé›†\")\n",
    "            else:\n",
    "                print(\"ä¼ªæ ‡ç­¾æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡ä½¿ç”¨\")\n",
    "\n",
    "        # åˆ›å»ºæ— æ ‡ç­¾æ•°æ®è¿­ä»£å™¨\n",
    "        unlabeled_iter = iter(self.unlabeled_loader)\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # æœ‰ç›‘ç£æŸå¤±\n",
    "            output = self.model(data)\n",
    "            supervised_loss = self.criterion(output, target)\n",
    "\n",
    "            total_loss = supervised_loss\n",
    "\n",
    "            # æ— ç›‘ç£æŸå¤±ï¼ˆä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼‰\n",
    "            consistency_loss = torch.tensor(0.0)\n",
    "            if use_consistency and current_consistency_weight > 0:\n",
    "                try:\n",
    "                    unlabeled_data, _ = next(unlabeled_iter)\n",
    "                    unlabeled_data = unlabeled_data.to(self.device)\n",
    "                    consistency_loss = self.consistency_loss(unlabeled_data)\n",
    "                    total_loss = supervised_loss + current_consistency_weight * consistency_loss\n",
    "                except StopIteration:\n",
    "                    unlabeled_iter = iter(self.unlabeled_loader)\n",
    "\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "            # è®¡ç®—å‡†ç¡®ç‡\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "            train_top5_correct += self.calculate_topk_accuracy(output, target, k=5)\n",
    "\n",
    "            # å®šæœŸæ‰“å°è®­ç»ƒä¿¡æ¯\n",
    "            if batch_idx % 50 == 0:\n",
    "                cons_loss_val = consistency_loss.item() if use_consistency else 0.0\n",
    "                print(f'Epoch: {epoch} | Batch: {batch_idx}/{len(self.train_loader)} | '\n",
    "                      f'æ€»æŸå¤±: {total_loss.item():.4f} | æœ‰ç›‘ç£: {supervised_loss.item():.4f} | '\n",
    "                      f'ä¸€è‡´æ€§: {cons_loss_val:.4f} | æƒé‡: {current_consistency_weight:.4f}')\n",
    "\n",
    "        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡\n",
    "        avg_train_loss = train_loss / len(self.train_loader)\n",
    "        train_accuracy = 100.0 * train_correct / train_total\n",
    "        train_top5_accuracy = 100.0 * train_top5_correct / train_total\n",
    "\n",
    "        return avg_train_loss, train_accuracy, train_top5_accuracy\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"éªŒè¯æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_top5_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.valid_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "                val_top5_correct += self.calculate_topk_accuracy(output, target, k=5)\n",
    "\n",
    "        avg_val_loss = val_loss / len(self.valid_loader)\n",
    "        val_accuracy = 100.0 * val_correct / val_total\n",
    "        val_top5_accuracy = 100.0 * val_top5_correct / val_total\n",
    "\n",
    "        return avg_val_loss, val_accuracy, val_top5_accuracy\n"
   ],
   "id": "32d9264e55063250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# è®­ç»ƒé…ç½®",
   "id": "d0709a72944d7b18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# åŸºæœ¬å‚æ•°\n",
    "BASE_DIR = os.getcwd() \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# åˆ›å»ºç»“æœç›®å½•\n",
    "now_time = datetime.now()\n",
    "time_str = datetime.strftime(now_time, '%m-%d_%H-%M')\n",
    "log_dir = os.path.join(BASE_DIR, \"results\", time_str)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "print(f\"ç»“æœä¿å­˜ç›®å½•: {log_dir}\")\n",
    "\n",
    "# è¶…å‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰\n",
    "MAX_EPOCH = 200\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "PATIENCE = 25  # å¢åŠ è€å¿ƒå€¼\n",
    "\n",
    "# åŠç›‘ç£å‚æ•°ä¼˜åŒ–\n",
    "PSEUDO_THRESHOLD = 0.90  # ç¨å¾®é™ä½é˜ˆå€¼\n",
    "CONSISTENCY_WEIGHT = 0.2  # å¢åŠ ä¸€è‡´æ€§æƒé‡\n",
    "\n",
    "print(\"è®­ç»ƒé…ç½®å®Œæˆ\")"
   ],
   "id": "5bbf480bd97e53ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# æ•°æ®åŠ è½½",
   "id": "d0da8ae0321bb312"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## æ•°æ®å¢å¼º",
   "id": "1337792d02b152ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# æ•°æ®å¢å¼º\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomCrop(512, padding=16),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "faedd3f32064e9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## åŠ è½½æ•°æ®",
   "id": "8afcd406e8dd582b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# åŠ è½½æ•°æ®é›†\n",
    "train_set = DatasetFolder(\n",
    "    os.path.join(BASE_DIR, \"food-11\", \"training\", \"labeled\"),\n",
    "    loader=pil_loader,\n",
    "    extensions=\"jpg\",\n",
    "    transform=train_tfm\n",
    ")\n",
    "\n",
    "valid_set = DatasetFolder(\n",
    "    os.path.join(BASE_DIR, \"food-11\", \"validation\"),\n",
    "    loader=pil_loader,\n",
    "    extensions=\"jpg\",\n",
    "    transform=test_tfm\n",
    ")\n",
    "\n",
    "unlabeled_set = DatasetFolder(\n",
    "    os.path.join(BASE_DIR, \"food-11\", \"training\", \"unlabeled\"),\n",
    "    loader=pil_loader,\n",
    "    extensions=\"jpg\",\n",
    "    transform=train_tfm\n",
    ")\n",
    "print(f\"è®­ç»ƒé›†: {len(train_set)}\")\n",
    "print(f\"æ— æ ‡ç­¾é›†: {len(unlabeled_set)}\")\n",
    "print(f\"éªŒè¯é›†: {len(valid_set)}\")"
   ],
   "id": "fd981c7043bce71a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## æ•°æ®åŠ è½½å™¨",
   "id": "e19b4a6abe4b057e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# æ•°æ®åŠ è½½å™¨\n",
    "num_workers = 2 if os.name == 'nt' else 4\n",
    "pin_memory = (device.type == 'cuda')\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "unlabeled_loader = DataLoader(\n",
    "    unlabeled_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "print(\"æ•°æ®åŠ è½½å®Œæˆ\")"
   ],
   "id": "27f572e2ae18c664"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# æ¨¡å‹åˆå§‹åŒ–",
   "id": "a79aa1f47051ddc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = resnet18_512(num_classes=11)\n",
    "model.to(device)"
   ],
   "id": "fe19fc5d4940eb65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "175051a8c5db3d6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨",
   "id": "e3fc39439f3be836"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "# ä½¿ç”¨CosineAnnealingWarmRestartså­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=50, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(f\"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "2d833c466d04f20a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# åˆ›å»ºè®­ç»ƒå™¨",
   "id": "e8556aa82884dffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = SemiSupervisedTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    unlabeled_loader=unlabeled_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    pseudo_threshold=PSEUDO_THRESHOLD,\n",
    "    consistency_weight=CONSISTENCY_WEIGHT\n",
    ")\n",
    "\n",
    "print(\"è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ\")"
   ],
   "id": "1a013e679e053563"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# å¼€å§‹è®­ç»ƒ",
   "id": "d4ea06be07df024e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## åˆå§‹åŒ–è®°å½•å˜é‡",
   "id": "445fc2ccd096f401"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_top5_accuracies = []\n",
    "val_top5_accuracies = []\n",
    "learning_rates = []\n",
    "best_val_accuracy = 0.0\n",
    "early_stop_counter = 0\n",
    "best_epoch = 0"
   ],
   "id": "59d0c195c30ce31e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## æ­£å¼å¼€å§‹è®­ç»ƒ",
   "id": "a26788dd8198a762"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"å¼€å§‹åŠç›‘ç£è®­ç»ƒ...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    # è®­ç»ƒé˜¶æ®µ\n",
    "    train_loss, train_accuracy, train_top5_accuracy = trainer.train_epoch(\n",
    "        epoch,\n",
    "        use_consistency=True,\n",
    "        use_pseudo_labels=True\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_top5_accuracies.append(train_top5_accuracy)\n",
    "\n",
    "    # æ›´æ–°å­¦ä¹ ç‡å¹¶è®°å½•\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    learning_rates.append(current_lr)\n",
    "    scheduler.step()\n",
    "\n",
    "    # éªŒè¯é˜¶æ®µ\n",
    "    val_loss, val_accuracy, val_top5_accuracy = trainer.validate()\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_top5_accuracies.append(val_top5_accuracy)\n",
    "\n",
    "    # æ—©åœåˆ¤æ–­å’Œæ¨¡å‹ä¿å­˜\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_accuracy\": best_val_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"train_loss\": train_loss\n",
    "        }\n",
    "        path_checkpoint = os.path.join(log_dir, \"checkpoint_best.pkl\")\n",
    "        torch.save(checkpoint, path_checkpoint)\n",
    "        print(f\"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    # ä¿å­˜æ¥è¿‘æœ€ä½³çš„æ¨¡å‹ï¼ˆç”¨äºé›†æˆï¼‰\n",
    "    elif val_accuracy > best_val_accuracy - 2.0:\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"epoch\": epoch\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(log_dir, f\"checkpoint_epoch_{epoch}_acc_{val_accuracy:.2f}.pth\"))\n",
    "\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    # æ‰“å°è®­ç»ƒä¿¡æ¯\n",
    "    print(f'Epoch: {epoch:03d}/{MAX_EPOCH}, '\n",
    "          f'è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.2f}% (Top-5: {train_top5_accuracy:.2f}%), '\n",
    "          f'éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}% (Top-5: {val_top5_accuracy:.2f}%), '\n",
    "          f'å­¦ä¹ ç‡: {current_lr:.6f}, '\n",
    "          f'æœ€ä½³: {best_val_accuracy:.2f}% @ Epoch {best_epoch}, '\n",
    "          f'æ—©åœè®¡æ•°: {early_stop_counter}/{PATIENCE}')\n",
    "    print('-' * 80)\n",
    "\n",
    "    # æ—©åœæ£€æŸ¥\n",
    "    if early_stop_counter >= PATIENCE:\n",
    "        print(f\"ğŸš¨ æ—©åœè§¦å‘ï¼åœ¨ epoch {epoch} åœæ­¢è®­ç»ƒ\")\n",
    "        print(f\"ğŸ† æœ€ä½³æ¨¡å‹åœ¨ epoch {best_epoch}, éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%\")\n",
    "        break\n",
    "\n",
    "# è®­ç»ƒå®Œæˆ\n",
    "print(f\"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%\")"
   ],
   "id": "fe8f0bf5329b53fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ä¿å­˜è®­ç»ƒè®°å½•",
   "id": "36820c82c2c6e217"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'train_top5_accuracies': train_top5_accuracies,\n",
    "    'val_top5_accuracies': val_top5_accuracies,\n",
    "    'learning_rates': learning_rates,\n",
    "    'best_val_accuracy': best_val_accuracy,\n",
    "    'best_epoch': best_epoch,\n",
    "    'pseudo_label_stats': trainer.pseudo_label_stats\n",
    "}\n",
    "\n",
    "torch.save(training_history, os.path.join(log_dir, 'training_history.pth'))\n",
    "\n",
    "# ç»˜åˆ¶è®­ç»ƒæ›²çº¿\n",
    "picture_path_loss = os.path.join(log_dir, 'loss_curves.png')\n",
    "picture_path_acc = os.path.join(log_dir, 'accuracy_curves.png')\n",
    "picture_path_combined = os.path.join(log_dir, 'training_curves.png')\n",
    "\n",
    "plot_loss_curves(train_losses, val_losses, picture_path_loss)\n",
    "plot_accuracy_curves(train_accuracies, val_accuracies, picture_path_acc)\n",
    "plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, picture_path_combined)\n",
    "\n",
    "print(f\"è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {log_dir}\")"
   ],
   "id": "9b16ee94b2634f3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ä¼ªæ ‡ç­¾ç»Ÿè®¡",
   "id": "9ca5cf73a1fd218d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if trainer.pseudo_label_stats:\n",
    "    pseudo_df = pd.DataFrame(trainer.pseudo_label_stats)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(pseudo_df['epoch'], pseudo_df['count'], 'b-o')\n",
    "    plt.title('ä¼ªæ ‡ç­¾æ•°é‡å˜åŒ–')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('æ•°é‡')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(pseudo_df['epoch'], pseudo_df['selection_rate'], 'g-o')\n",
    "    plt.title('ä¼ªæ ‡ç­¾é€‰æ‹©ç‡')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('é€‰æ‹©ç‡')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(pseudo_df['epoch'], pseudo_df['avg_confidence'], 'r-o')\n",
    "    plt.title('ä¼ªæ ‡ç­¾å¹³å‡ç½®ä¿¡åº¦')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ç½®ä¿¡åº¦')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(pseudo_df['epoch'], pseudo_df['threshold'], 'purple-o')\n",
    "    plt.title('åŠ¨æ€é˜ˆå€¼å˜åŒ–')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('é˜ˆå€¼')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(log_dir, 'pseudo_label_stats.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "caab9f9eea5a1828"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# æœ€ç»ˆç»“æœç»Ÿè®¡",
   "id": "3df257c9293fcec8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"è®­ç»ƒæ€»ç»“:\")\n",
    "print(f\"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}% (Epoch {best_epoch})\")\n",
    "print(f\"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_accuracies[-1]:.2f}%\")\n",
    "print(f\"è®­ç»ƒè½®æ•°: {len(train_accuracies)}\")\n",
    "print(f\"ç»“æœä¿å­˜ç›®å½•: {log_dir}\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "52b1e8985cacb07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
