{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 下载数据",
   "id": "ffae4c446514d3d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# You may choose where to download the data.\n",
    "\n",
    "# Google Drive\n",
    "!gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip\n",
    "\n",
    "# Dropbox\n",
    "# !wget https://www.dropbox.com/s/m9q6273jl3djall/food-11.zip -O food-11.zip\n",
    "\n",
    "# MEGA\n",
    "# !sudo apt install megatools\n",
    "# !megadl \"https://mega.nz/#!zt1TTIhK!ZuMbg5ZjGWzWX1I6nEUbfjMZgCmAgeqJlwDkqdIryfg\"\n",
    "\n",
    "# Unzip the dataset.\n",
    "# This may take some time.\n",
    "!unzip -q food-11.zip"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 使用半监督学习",
   "id": "91ebae5e90dc8698"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 导入包",
   "id": "59663434c892e9e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:42.205426Z",
     "start_time": "2025-10-19T03:04:42.202088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "sys.path.append('../tools')\n",
    "import pandas as pd"
   ],
   "id": "8e88a5ce39689d6c",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构建ResNet-18-512模型",
   "id": "200bba4391b24648"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:43.104105Z",
     "start_time": "2025-10-19T03:04:43.095596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 基础残差块\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# 适配512×512的ResNet-18模型\n",
    "class ResNet18_512(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=11, num_channels=3):\n",
    "        super(ResNet18_512, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # 初始卷积层 - 针对512×512调整\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # 残差层\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # 分类器 - 添加Dropout防止过拟合\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)  # 添加Dropout\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # 权重初始化\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入: 512×512\n",
    "        x = self.conv1(x)    # 512→256\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)  # 256→128\n",
    "\n",
    "        x = self.layer1(x)   # 128→128\n",
    "        x = self.layer2(x)   # 128→64\n",
    "        x = self.layer3(x)   # 64→32\n",
    "        x = self.layer4(x)   # 32→16\n",
    "\n",
    "        x = self.avgpool(x)  # 16→1\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)  # 添加Dropout\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18_512(num_classes=11):\n",
    "    \"\"\"创建适配512×512的ResNet-18模型\"\"\"\n",
    "    return ResNet18_512(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n"
   ],
   "id": "c0033f633dc78545",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:43.402218Z",
     "start_time": "2025-10-19T03:04:43.321341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = resnet18_512(num_classes=11)\n",
    "print(model)"
   ],
   "id": "a509b1187c23d364",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18_512(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 导入common tools",
   "id": "6ef5db417972f192"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:44.735007Z",
     "start_time": "2025-10-19T03:04:44.728986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_loss_curves(train_losses, val_losses, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制训练和验证损失曲线\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "    plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"图片已保存到: {save_path}\")\n"
   ],
   "id": "6bbe21bab1db3576",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:45.175209Z",
     "start_time": "2025-10-19T03:04:45.170666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_accuracy_curves(train_accuracies, val_accuracies, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制准确率曲线\n",
    "\n",
    "    Args:\n",
    "        train_accuracies: 训练准确率列表\n",
    "        val_accuracies: 验证准确率列表\n",
    "        save_path: 图片保存路径\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    epochs = range(1, len(train_accuracies) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "\n",
    "    plt.title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epochs', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # 设置y轴范围从0开始\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "    # 添加最佳准确率标注\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_epoch = val_accuracies.index(best_val_acc) + 1\n",
    "    plt.axvline(x=best_epoch, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.text(best_epoch, best_val_acc / 2, f'Best: {best_val_acc:.2f}%\\nEpoch: {best_epoch}',\n",
    "             ha='center', va='center', fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ],
   "id": "dea168784378a938",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:45.855328Z",
     "start_time": "2025-10-19T03:04:45.849375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制综合训练曲线（损失和准确率在一起）\n",
    "\n",
    "    Args:\n",
    "        train_losses: 训练损失列表\n",
    "        val_losses: 验证损失列表\n",
    "        train_accuracies: 训练准确率列表\n",
    "        val_accuracies: 验证准确率列表\n",
    "        save_path: 图片保存路径\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # 绘制损失曲线\n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 绘制准确率曲线\n",
    "    ax2.plot(epochs, train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epochs', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax2.legend(fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(bottom=0)\n",
    "\n",
    "    # 添加最佳准确率标注\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_epoch = val_accuracies.index(best_val_acc) + 1\n",
    "    ax2.axvline(x=best_epoch, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax2.text(best_epoch, best_val_acc / 2, f'Best: {best_val_acc:.2f}%',\n",
    "             ha='center', va='center', fontsize=10,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ],
   "id": "cbb5bae9ac0f3551",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练",
   "id": "d7ff42654890d1f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 导入包",
   "id": "88c5e983a5c24937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:50.717731Z",
     "start_time": "2025-10-19T03:04:50.714573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n"
   ],
   "id": "2c22cf24233bd9d7",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 半监督学习添加的函数",
   "id": "da2f2f2599d2d55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:51.058753Z",
     "start_time": "2025-10-19T03:04:51.055505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义可序列化的图片加载函数\n",
    "def pil_loader(path):\n",
    "    \"\"\"使用PIL加载图片，支持多种格式\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')  # 确保图片是RGB格式"
   ],
   "id": "c4046b3c0babbf96",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:51.264903Z",
     "start_time": "2025-10-19T03:04:51.252903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SemiSupervisedTrainer:\n",
    "    \"\"\"\n",
    "    半监督训练器类\n",
    "    结合有标签数据和无标签数据进行训练，提升模型性能\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, train_loader, unlabeled_loader, valid_loader,\n",
    "                 optimizer, criterion, device, pseudo_threshold=0.9, consistency_weight=0.3):\n",
    "        # 初始化模型和数据加载器\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader  # 有标签训练数据\n",
    "        self.unlabeled_loader = unlabeled_loader  # 无标签数据\n",
    "        self.valid_loader = valid_loader  # 验证数据\n",
    "        self.optimizer = optimizer  # 优化器\n",
    "        self.criterion = criterion  # 损失函数\n",
    "        self.device = device  # 训练设备\n",
    "        self.pseudo_threshold = pseudo_threshold  # 伪标签置信度阈值\n",
    "        self.consistency_weight = consistency_weight  # 一致性损失权重\n",
    "\n",
    "        # 数据增强（用于一致性训练）\n",
    "        # 弱增强：轻微的数据变换，保持图像主要内容不变\n",
    "        self.weak_augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # 随机水平翻转\n",
    "            transforms.RandomRotation(degrees=10),  # 随机旋转\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 颜色抖动\n",
    "        ])\n",
    "\n",
    "        # 强增强：更强的数据变换，产生更多样化的图像\n",
    "        self.strong_augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),  # 更强的颜色抖动\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # 仿射变换\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # 高斯模糊\n",
    "        ])\n",
    "\n",
    "    def generate_pseudo_labels(self, confidence_threshold=None):\n",
    "        \"\"\"生成伪标签：使用当前模型对无标签数据进行预测，选择高置信度的预测作为伪标签\"\"\"\n",
    "        if confidence_threshold is None:\n",
    "            confidence_threshold = self.pseudo_threshold\n",
    "\n",
    "        self.model.eval()  # 设置为评估模式\n",
    "        pseudo_data = []  # 存储伪标签数据\n",
    "        pseudo_labels = []  # 存储伪标签\n",
    "\n",
    "        with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "            for data, _ in self.unlabeled_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                probabilities = F.softmax(outputs, dim=1)  # 转换为概率分布\n",
    "                max_probs, predictions = torch.max(probabilities, 1)  # 获取最大概率和预测类别\n",
    "\n",
    "                # 筛选高置信度样本：只选择置信度高于阈值的预测\n",
    "                mask = max_probs > confidence_threshold\n",
    "                high_conf_data = data[mask].cpu()  # 高置信度数据\n",
    "                high_conf_preds = predictions[mask].cpu()  # 对应的伪标签\n",
    "\n",
    "                if len(high_conf_data) > 0:\n",
    "                    for i in range(len(high_conf_data)):\n",
    "                        pseudo_data.append(high_conf_data[i])\n",
    "                        pseudo_labels.append(high_conf_preds[i])\n",
    "\n",
    "        print(f\"生成了 {len(pseudo_data)} 个伪标签样本 (阈值: {confidence_threshold})\")\n",
    "        return pseudo_data, pseudo_labels\n",
    "\n",
    "    def consistency_loss(self, unlabeled_batch):\n",
    "        \"\"\"计算一致性损失：对同一无标签数据应用不同增强，期望模型输出一致的预测\"\"\"\n",
    "        batch_size = unlabeled_batch.size(0)\n",
    "\n",
    "        # 弱增强：保持图像主要内容\n",
    "        weak_aug = self.weak_augment(unlabeled_batch)\n",
    "\n",
    "        # 强增强：更强的图像变换\n",
    "        strong_aug = self.strong_augment(unlabeled_batch)\n",
    "\n",
    "        # 获取预测\n",
    "        with torch.no_grad():\n",
    "            weak_output = F.softmax(self.model(weak_aug), dim=1)  # 弱增强的预测作为\"教师\"\n",
    "\n",
    "        strong_output = F.log_softmax(self.model(strong_aug), dim=1)  # 强增强的预测作为\"学生\"\n",
    "\n",
    "        # 计算KL散度损失：衡量两个概率分布的差异\n",
    "        consistency_loss = F.kl_div(strong_output, weak_output, reduction='batchmean')\n",
    "        return consistency_loss\n",
    "\n",
    "    def train_epoch(self, epoch, use_consistency=True, use_pseudo_labels=False):\n",
    "        \"\"\"训练一个epoch：结合有监督损失和无监督损失\"\"\"\n",
    "        self.model.train()  # 设置为训练模式\n",
    "        train_loss = 0.0  # 累计训练损失\n",
    "        train_correct = 0  # 正确预测数量\n",
    "        train_total = 0  # 总样本数量\n",
    "\n",
    "        # 伪标签生成：定期使用当前模型生成伪标签来扩展训练集\n",
    "        if use_pseudo_labels and epoch % 5 == 0 and epoch > 10:\n",
    "            pseudo_data, pseudo_labels = self.generate_pseudo_labels()\n",
    "            if pseudo_data:\n",
    "                # 创建伪标签数据集（这里简化处理，实际应该创建完整的Dataset）\n",
    "                pseudo_dataset = list(zip(pseudo_data, pseudo_labels))\n",
    "                print(\"使用伪标签数据扩展训练集\")\n",
    "\n",
    "        # 创建无标签数据迭代器\n",
    "        unlabeled_iter = iter(self.unlabeled_loader)\n",
    "\n",
    "        # 遍历有标签训练数据\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "            # 有监督损失：使用有标签数据计算的标准交叉熵损失\n",
    "            output = self.model(data)\n",
    "            supervised_loss = self.criterion(output, target)\n",
    "\n",
    "            total_loss = supervised_loss  # 总损失初始化为有监督损失\n",
    "\n",
    "            # 无监督损失（一致性正则化）：使用无标签数据计算的一致性损失\n",
    "            if use_consistency:\n",
    "                try:\n",
    "                    unlabeled_data, _ = next(unlabeled_iter)\n",
    "                    unlabeled_data = unlabeled_data.to(self.device)\n",
    "\n",
    "                    consistency_loss = self.consistency_loss(unlabeled_data)\n",
    "                    # 组合损失：有监督损失 + 权重 * 无监督损失\n",
    "                    total_loss = supervised_loss + self.consistency_weight * consistency_loss\n",
    "\n",
    "                except StopIteration:\n",
    "                    # 重置迭代器：当无标签数据遍历完时重新开始\n",
    "                    unlabeled_iter = iter(self.unlabeled_loader)\n",
    "                    consistency_loss = torch.tensor(0.0)\n",
    "\n",
    "            total_loss.backward()  # 反向传播计算梯度\n",
    "            self.optimizer.step()  # 更新模型参数\n",
    "\n",
    "            train_loss += total_loss.item()  # 累计损失\n",
    "\n",
    "            # 计算训练准确率\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "            # 定期打印训练信息\n",
    "            if batch_idx % 50 == 0:\n",
    "                cons_loss_val = consistency_loss.item() if use_consistency else 0.0\n",
    "                print(f'Epoch: {epoch} | Batch: {batch_idx}/{len(self.train_loader)} | '\n",
    "                      f'Total Loss: {total_loss.item():.4f} | Supervised: {supervised_loss.item():.4f} | '\n",
    "                      f'Consistency: {cons_loss_val:.4f}')\n",
    "\n",
    "        # 计算平均训练损失和准确率\n",
    "        avg_train_loss = train_loss / len(self.train_loader)\n",
    "        train_accuracy = 100.0 * train_correct / train_total\n",
    "\n",
    "        return avg_train_loss, train_accuracy\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"验证模型性能：在验证集上评估模型\"\"\"\n",
    "        self.model.eval()  # 设置为评估模式\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():  # 禁用梯度计算\n",
    "            for data, target in self.valid_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(self.valid_loader)\n",
    "        val_accuracy = 100.0 * val_correct / val_total\n",
    "\n",
    "        return avg_val_loss, val_accuracy\n"
   ],
   "id": "481a45f61b1047e4",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 基本训练参数",
   "id": "7c7b5dba824c1eb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:51.597951Z",
     "start_time": "2025-10-19T03:04:51.594773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BASE_DIR = os.getcwd() \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 自动选择GPU或CPU\n",
    "parent_dir = os.path.dirname(BASE_DIR)  # 获取上级文件夹\n",
    "pin_memory = (device.type == 'cuda')  # 如果使用GPU，启用内存锁页加速数据传输"
   ],
   "id": "cda1d831c0e9b056",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:51.788083Z",
     "start_time": "2025-10-19T03:04:51.784633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "now_time = datetime.now()\n",
    "time_str = datetime.strftime(now_time, '%m-%d_%H-%M')\n",
    "log_dir = os.path.join(BASE_DIR, \"..\", \"results\", time_str)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ],
   "id": "6afb4849ff857fc5",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 超参",
   "id": "9e2f08bb454c7638"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:52.113271Z",
     "start_time": "2025-10-19T03:04:52.110756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_EPOCH = 182  # 最大训练轮数\n",
    "BATCH_SIZE = 32  # 批大小\n",
    "LR = 0.001  # 学习率\n",
    "PATIENCE = 20  # 早停耐心值\n",
    "milestones = [92, 136]  # 学习率调整的里程碑\n",
    "\n",
    "# 半监督参数\n",
    "PSEUDO_THRESHOLD = 0.9  # 伪标签置信度阈值\n",
    "CONSISTENCY_WEIGHT = 0.3  # 一致性损失权重\n"
   ],
   "id": "9ff987453236565f",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据加载",
   "id": "70bdc6ed5a9ce4ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:52.907327Z",
     "start_time": "2025-10-19T03:04:52.902695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练数据增强：使用多种数据增强技术提升模型泛化能力\n",
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # 调整图像大小\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 随机水平翻转\n",
    "    transforms.RandomRotation(degrees=15),  # 随机旋转\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),  # 颜色抖动\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # 仿射变换\n",
    "    transforms.RandomCrop(512, padding=16),  # 随机裁剪\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "])\n",
    "\n",
    "# 测试/验证数据增强：只进行必要的预处理\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "aca7b29361f5ef8e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:53.494785Z",
     "start_time": "2025-10-19T03:04:53.454519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载数据集\n",
    "# 有标签训练集\n",
    "train_set = DatasetFolder(\n",
    "    os.path.join(BASE_DIR, \"food-11\", \"training\", \"labeled\"),\n",
    "    loader=pil_loader,\n",
    "    extensions=\"jpg\",\n",
    "    transform=train_tfm\n",
    ")\n",
    "\n",
    "# 验证集\n",
    "valid_set = DatasetFolder(\n",
    "    os.path.join(BASE_DIR, \"food-11\", \"validation\"),\n",
    "    loader=pil_loader,\n",
    "    extensions=\"jpg\",\n",
    "    transform=test_tfm\n",
    ")\n",
    "\n",
    "# 无标签数据集\n",
    "unlabeled_set = DatasetFolder(\n",
    "    os.path.join(BASE_DIR, \"food-11\", \"training\", \"unlabeled\"),\n",
    "    loader=pil_loader,\n",
    "    extensions=\"jpg\",\n",
    "    transform=train_tfm  # 训练时增强\n",
    ")\n",
    "\n",
    "# 测试集\n",
    "test_set = DatasetFolder(\n",
    "    os.path.join(BASE_DIR, \"food-11\", \"testing\"),\n",
    "    loader=pil_loader,\n",
    "    extensions=\"jpg\",\n",
    "    transform=test_tfm\n",
    ")"
   ],
   "id": "c1c11373441cf9bd",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:05:36.825286Z",
     "start_time": "2025-10-19T03:05:36.819996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"训练集: {len(train_set)}\")\n",
    "print(f\"无标签集: {len(unlabeled_set)}\")\n",
    "print(f\"验证集: {len(valid_set)}\")\n",
    "print(f\"使用设备: {device}\")\n"
   ],
   "id": "afcdadef5837edbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 3080\n",
      "无标签集: 6786\n",
      "验证集: 660\n",
      "使用设备: cpu\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:53.899208Z",
     "start_time": "2025-10-19T03:04:53.894302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据加载器\n",
    "num_workers = 2 if os.name == 'nt' else 4  # Windows系统使用较少进程\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # 训练时打乱数据\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "unlabeled_loader = DataLoader(\n",
    "    unlabeled_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # 训练时打乱数据\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,  # 验证时不需要打乱\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory\n",
    ")\n"
   ],
   "id": "3996a109225750dd",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 半监督学习训练器",
   "id": "df0dc0da73103347"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:04:54.944880Z",
     "start_time": "2025-10-19T03:04:54.931940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SemiSupervisedTrainer:\n",
    "    \"\"\"\n",
    "    半监督训练器类\n",
    "    结合有标签数据和无标签数据进行训练，提升模型性能\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, train_loader, unlabeled_loader, valid_loader,\n",
    "                 optimizer, criterion, device, pseudo_threshold=0.9, consistency_weight=0.3):\n",
    "        # 初始化模型和数据加载器\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader  # 有标签训练数据\n",
    "        self.unlabeled_loader = unlabeled_loader  # 无标签数据\n",
    "        self.valid_loader = valid_loader  # 验证数据\n",
    "        self.optimizer = optimizer  # 优化器\n",
    "        self.criterion = criterion  # 损失函数\n",
    "        self.device = device  # 训练设备\n",
    "        self.pseudo_threshold = pseudo_threshold  # 伪标签置信度阈值\n",
    "        self.consistency_weight = consistency_weight  # 一致性损失权重\n",
    "\n",
    "        # 数据增强（用于一致性训练）\n",
    "        # 弱增强：轻微的数据变换，保持图像主要内容不变\n",
    "        self.weak_augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # 随机水平翻转\n",
    "            transforms.RandomRotation(degrees=10),  # 随机旋转\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 颜色抖动\n",
    "        ])\n",
    "\n",
    "        # 强增强：更强的数据变换，产生更多样化的图像\n",
    "        self.strong_augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),  # 更强的颜色抖动\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # 仿射变换\n",
    "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # 高斯模糊\n",
    "        ])\n",
    "\n",
    "    def generate_pseudo_labels(self, confidence_threshold=None):\n",
    "        \"\"\"生成伪标签：使用当前模型对无标签数据进行预测，选择高置信度的预测作为伪标签\"\"\"\n",
    "        if confidence_threshold is None:\n",
    "            confidence_threshold = self.pseudo_threshold\n",
    "\n",
    "        self.model.eval()  # 设置为评估模式\n",
    "        pseudo_data = []  # 存储伪标签数据\n",
    "        pseudo_labels = []  # 存储伪标签\n",
    "\n",
    "        with torch.no_grad():  # 禁用梯度计算，节省内存\n",
    "            for data, _ in self.unlabeled_loader:\n",
    "                data = data.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                probabilities = F.softmax(outputs, dim=1)  # 转换为概率分布\n",
    "                max_probs, predictions = torch.max(probabilities, 1)  # 获取最大概率和预测类别\n",
    "\n",
    "                # 筛选高置信度样本：只选择置信度高于阈值的预测\n",
    "                mask = max_probs > confidence_threshold\n",
    "                high_conf_data = data[mask].cpu()  # 高置信度数据\n",
    "                high_conf_preds = predictions[mask].cpu()  # 对应的伪标签\n",
    "\n",
    "                if len(high_conf_data) > 0:\n",
    "                    for i in range(len(high_conf_data)):\n",
    "                        pseudo_data.append(high_conf_data[i])\n",
    "                        pseudo_labels.append(high_conf_preds[i])\n",
    "\n",
    "        print(f\"生成了 {len(pseudo_data)} 个伪标签样本 (阈值: {confidence_threshold})\")\n",
    "        return pseudo_data, pseudo_labels\n",
    "\n",
    "    def consistency_loss(self, unlabeled_batch):\n",
    "        \"\"\"计算一致性损失：对同一无标签数据应用不同增强，期望模型输出一致的预测\"\"\"\n",
    "        batch_size = unlabeled_batch.size(0)\n",
    "\n",
    "        # 弱增强：保持图像主要内容\n",
    "        weak_aug = self.weak_augment(unlabeled_batch)\n",
    "\n",
    "        # 强增强：更强的图像变换\n",
    "        strong_aug = self.strong_augment(unlabeled_batch)\n",
    "\n",
    "        # 获取预测\n",
    "        with torch.no_grad():\n",
    "            weak_output = F.softmax(self.model(weak_aug), dim=1)  # 弱增强的预测作为\"教师\"\n",
    "\n",
    "        strong_output = F.log_softmax(self.model(strong_aug), dim=1)  # 强增强的预测作为\"学生\"\n",
    "\n",
    "        # 计算KL散度损失：衡量两个概率分布的差异\n",
    "        consistency_loss = F.kl_div(strong_output, weak_output, reduction='batchmean')\n",
    "        return consistency_loss\n",
    "\n",
    "    def train_epoch(self, epoch, use_consistency=True, use_pseudo_labels=False):\n",
    "        \"\"\"训练一个epoch：结合有监督损失和无监督损失\"\"\"\n",
    "        self.model.train()  # 设置为训练模式\n",
    "        train_loss = 0.0  # 累计训练损失\n",
    "        train_correct = 0  # 正确预测数量\n",
    "        train_total = 0  # 总样本数量\n",
    "\n",
    "        # 伪标签生成：定期使用当前模型生成伪标签来扩展训练集\n",
    "        if use_pseudo_labels and epoch % 5 == 0 and epoch > 10:\n",
    "            pseudo_data, pseudo_labels = self.generate_pseudo_labels()\n",
    "            if pseudo_data:\n",
    "                # 创建伪标签数据集（这里简化处理，实际应该创建完整的Dataset）\n",
    "                pseudo_dataset = list(zip(pseudo_data, pseudo_labels))\n",
    "                print(\"使用伪标签数据扩展训练集\")\n",
    "\n",
    "        # 创建无标签数据迭代器\n",
    "        unlabeled_iter = iter(self.unlabeled_loader)\n",
    "\n",
    "        # 遍历有标签训练数据\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "            # 有监督损失：使用有标签数据计算的标准交叉熵损失\n",
    "            output = self.model(data)\n",
    "            supervised_loss = self.criterion(output, target)\n",
    "\n",
    "            total_loss = supervised_loss  # 总损失初始化为有监督损失\n",
    "\n",
    "            # 无监督损失（一致性正则化）：使用无标签数据计算的一致性损失\n",
    "            if use_consistency:\n",
    "                try:\n",
    "                    unlabeled_data, _ = next(unlabeled_iter)\n",
    "                    unlabeled_data = unlabeled_data.to(self.device)\n",
    "\n",
    "                    consistency_loss = self.consistency_loss(unlabeled_data)\n",
    "                    # 组合损失：有监督损失 + 权重 * 无监督损失\n",
    "                    total_loss = supervised_loss + self.consistency_weight * consistency_loss\n",
    "\n",
    "                except StopIteration:\n",
    "                    # 重置迭代器：当无标签数据遍历完时重新开始\n",
    "                    unlabeled_iter = iter(self.unlabeled_loader)\n",
    "                    consistency_loss = torch.tensor(0.0)\n",
    "\n",
    "            total_loss.backward()  # 反向传播计算梯度\n",
    "            self.optimizer.step()  # 更新模型参数\n",
    "\n",
    "            train_loss += total_loss.item()  # 累计损失\n",
    "\n",
    "            # 计算训练准确率\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "\n",
    "            # 定期打印训练信息\n",
    "            if batch_idx % 50 == 0:\n",
    "                cons_loss_val = consistency_loss.item() if use_consistency else 0.0\n",
    "                print(f'Epoch: {epoch} | Batch: {batch_idx}/{len(self.train_loader)} | '\n",
    "                      f'Total Loss: {total_loss.item():.4f} | Supervised: {supervised_loss.item():.4f} | '\n",
    "                      f'Consistency: {cons_loss_val:.4f}')\n",
    "\n",
    "        # 计算平均训练损失和准确率\n",
    "        avg_train_loss = train_loss / len(self.train_loader)\n",
    "        train_accuracy = 100.0 * train_correct / train_total\n",
    "\n",
    "        return avg_train_loss, train_accuracy\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"验证模型性能：在验证集上评估模型\"\"\"\n",
    "        self.model.eval()  # 设置为评估模式\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():  # 禁用梯度计算\n",
    "            for data, target in self.valid_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(self.valid_loader)\n",
    "        val_accuracy = 100.0 * val_correct / val_total\n",
    "\n",
    "        return avg_val_loss, val_accuracy\n"
   ],
   "id": "e898e6140c43733e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型定义",
   "id": "7b339536cdb663e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:07:11.537892Z",
     "start_time": "2025-10-19T03:07:11.451940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = resnet18_512(num_classes=11)  # 使用ResNet-18模型，适配512x512输入\n",
    "# 或者使用: model = FoodCNN_2(num_classes=11)  # 轻量级自定义CNN\n",
    "model.to(device)  # 将模型移动到指定设备\n",
    "\n",
    "print(f\"模型已创建，移动到设备: {device}\")\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "id": "ab4d0026b8b3b9ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已创建，移动到设备: cpu\n",
      "模型参数量: 11,182,155\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 损失函数和优化器",
   "id": "761cbb9e18968a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:07:17.513248Z",
     "start_time": "2025-10-19T03:07:17.510075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================ 损失函数和优化器 ============================\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，用于分类任务\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)  # AdamW优化器\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, gamma=0.1, milestones=milestones)  # 多步学习率调度\n"
   ],
   "id": "e6770ab83320f811",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 创建训练器",
   "id": "737de8ca1c4cfc8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:07:47.139277Z",
     "start_time": "2025-10-19T03:07:47.134123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================ 创建训练器 ============================\n",
    "trainer = SemiSupervisedTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    unlabeled_loader=unlabeled_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    pseudo_threshold=PSEUDO_THRESHOLD,\n",
    "    consistency_weight=CONSISTENCY_WEIGHT\n",
    ")"
   ],
   "id": "5cb37d1f0d683450",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 循环训练开始",
   "id": "1c64734ba07b8d03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T03:08:23.803960Z",
     "start_time": "2025-10-19T03:08:23.800784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 初始化记录变量\n",
    "train_losses = []  # 训练损失记录\n",
    "val_losses = []  # 验证损失记录\n",
    "train_accuracies = []  # 训练准确率记录\n",
    "val_accuracies = []  # 验证准确率记录\n",
    "learning_rates = []  # 学习率记录\n",
    "best_val_accuracy = 0.0  # 最佳验证准确率\n",
    "early_stop_counter = 0  # 早停计数器\n",
    "best_epoch = 0  # 最佳模型所在轮数\n",
    "\n",
    "print(\"开始半监督训练...\")"
   ],
   "id": "8ce376b30fe017bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始半监督训练...\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-19T03:08:52.417130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(MAX_EPOCH):\n",
    "    # 训练阶段\n",
    "    train_loss, train_accuracy = trainer.train_epoch(\n",
    "        epoch,\n",
    "        use_consistency=True,  # 使用一致性训练\n",
    "        use_pseudo_labels=True  # 使用伪标签\n",
    "    )\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # 更新学习率并记录\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    learning_rates.append(current_lr)\n",
    "    scheduler.step()  # 更新学习率\n",
    "\n",
    "    # 验证阶段\n",
    "    val_loss, val_accuracy = trainer.validate()\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # 早停判断和模型保存\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch\n",
    "        early_stop_counter = 0  # 重置早停计数器\n",
    "\n",
    "        # 保存最佳模型\n",
    "        checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_val_accuracy\": best_val_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"train_loss\": train_loss\n",
    "        }\n",
    "        path_checkpoint = os.path.join(log_dir, \"checkpoint_best.pkl\")\n",
    "        torch.save(checkpoint, path_checkpoint)\n",
    "        print(f\"✅ 保存最佳模型，验证准确率: {best_val_accuracy:.2f}%\")\n",
    "    else:\n",
    "        early_stop_counter += 1  # 增加早停计数器\n",
    "\n",
    "    # 打印训练信息\n",
    "    print(f'Epoch: {epoch:03d}/{MAX_EPOCH}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, '\n",
    "          f'LR: {current_lr:.6f}, '\n",
    "          f'EarlyStop: {early_stop_counter}/{PATIENCE}')\n",
    "\n",
    "    # 早停检查：如果连续PATIENCE个epoch验证准确率没有提升，停止训练\n",
    "    if early_stop_counter >= PATIENCE:\n",
    "        print(f\"🚨 早停触发！在 epoch {epoch} 停止训练\")\n",
    "        print(f\"🏆 最佳模型在 epoch {best_epoch}, 验证准确率: {best_val_accuracy:.2f}%\")\n",
    "        break\n",
    "\n",
    "# ============================ 训练结束 ============================\n",
    "print(f\"训练完成！最终最佳验证准确率: {best_val_accuracy:.2f}%\")\n",
    "\n",
    "# 保存训练记录\n",
    "training_history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'learning_rates': learning_rates,\n",
    "    'best_val_accuracy': best_val_accuracy,\n",
    "    'best_epoch': best_epoch\n",
    "}\n",
    "torch.save(training_history, os.path.join(log_dir, 'training_history.pth'))\n",
    "\n",
    "# 绘制训练曲线\n",
    "picture_path_loss = os.path.join(log_dir, 'loss_curves.png')\n",
    "picture_path_acc = os.path.join(log_dir, 'accuracy_curves.png')\n",
    "picture_path_combined = os.path.join(log_dir, 'training_curves.png')\n",
    "\n",
    "plot_loss_curves(train_losses, val_losses, picture_path_loss)\n",
    "plot_accuracy_curves(train_accuracies, val_accuracies, picture_path_acc)\n",
    "plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, picture_path_combined)\n",
    "\n",
    "print(f\"训练曲线已保存至: {log_dir}\")"
   ],
   "id": "6d9f17b3f1be9257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6143a4539ee805f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
